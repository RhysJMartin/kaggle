{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width, img_height = 150, 150\n",
    "image_size = (img_width, img_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image data and labels\n",
    "- create test set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40479/40479 [02:00<00:00, 334.94it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train_v2.csv')\n",
    "\n",
    "with open('../label_maps.yml', 'r') as lablels:\n",
    "    label_data = yaml.load(lablels)\n",
    "inv_label_map = label_data['inv_label_map']\n",
    "label_map = label_data['label_map']\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for f, tags in tqdm(df_train.values, miniters=1000):\n",
    "    img = cv2.imread('../data/train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x_train.append(cv2.resize(img, image_size))\n",
    "    y_train.append(targets)\n",
    "    \n",
    "    \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "x_train = np.array(x_train, np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29245, 150, 150, 3)\n",
      "(29245, 17)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.15, random_state=42)\n",
    "nb_train_samples = len(x_train)\n",
    "nb_validation_samples = len(x_valid)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train = x_train[:10]\n",
    "#x_valid = x_valid[:5]\n",
    "#y_train = y_train[:10]\n",
    "#y_valid = y_valid[:5]\n",
    "#nb_train_samples = len(x_train)\n",
    "#nb_validation_samples = len(x_valid)\n",
    "#epochs = 10\n",
    "#batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the top layer\n",
    "- Creates feature vector\n",
    "- trains top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "    batch_size = 256\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    print('Loading VGG16 model')\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    # generator does not shuffle images, the output should be in the same order as the data\n",
    "    print('Starting process training data')\n",
    "    generator = datagen.flow(\n",
    "        x_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size, verbose = 1)\n",
    "    print(bottleneck_features_train.dtype)\n",
    "    np.save(open('../models/bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "    np.save(open('../models/bottleneck_labels_train.npy', 'wb'),\n",
    "            y_train)\n",
    "    \n",
    "    print('Starting process validation data')\n",
    "    generator = datagen.flow(\n",
    "        x_valid,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size, verbose = 1)\n",
    "    np.save(open('../models/bottleneck_features_validation.npy', 'wb'),\n",
    "            bottleneck_features_validation)\n",
    "    np.save(open('../models/bottleneck_labels_validation.npy', 'wb'),\n",
    "            y_valid)\n",
    "\n",
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train top model and save parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34304, 4, 4, 512)\n",
      "(4, 4, 512)\n",
      "(34304, 4, 4, 512)\n",
      "(34304, 17)\n",
      "(5888, 4, 4, 512)\n",
      "(5888, 17)\n",
      "Train on 34304 samples, validate on 5888 samples\n",
      "Epoch 1/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.2557 - acc: 0.9026 - val_loss: 0.2137 - val_acc: 0.9160\n",
      "Epoch 2/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.2184 - acc: 0.9153 - val_loss: 0.1985 - val_acc: 0.9235\n",
      "Epoch 3/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.2048 - acc: 0.9203 - val_loss: 0.1892 - val_acc: 0.9274\n",
      "Epoch 4/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1955 - acc: 0.9235 - val_loss: 0.1825 - val_acc: 0.9296\n",
      "Epoch 5/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1887 - acc: 0.9259 - val_loss: 0.1776 - val_acc: 0.9314\n",
      "Epoch 6/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1839 - acc: 0.9276 - val_loss: 0.1736 - val_acc: 0.9333\n",
      "Epoch 7/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1795 - acc: 0.9294 - val_loss: 0.1706 - val_acc: 0.9343\n",
      "Epoch 8/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1760 - acc: 0.9309 - val_loss: 0.1680 - val_acc: 0.9361\n",
      "Epoch 9/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1735 - acc: 0.9319 - val_loss: 0.1654 - val_acc: 0.9363\n",
      "Epoch 10/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1708 - acc: 0.9330 - val_loss: 0.1633 - val_acc: 0.9372\n",
      "Epoch 11/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1684 - acc: 0.9343 - val_loss: 0.1611 - val_acc: 0.9378\n",
      "Epoch 12/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1661 - acc: 0.9353 - val_loss: 0.1593 - val_acc: 0.9389\n",
      "Epoch 13/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1647 - acc: 0.9358 - val_loss: 0.1580 - val_acc: 0.9383\n",
      "Epoch 14/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1631 - acc: 0.9362 - val_loss: 0.1578 - val_acc: 0.9384\n",
      "Epoch 15/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1617 - acc: 0.9368 - val_loss: 0.1552 - val_acc: 0.9407\n",
      "Epoch 16/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1602 - acc: 0.9374 - val_loss: 0.1555 - val_acc: 0.9397\n",
      "Epoch 17/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1589 - acc: 0.9380 - val_loss: 0.1542 - val_acc: 0.9396\n",
      "Epoch 18/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1576 - acc: 0.9384 - val_loss: 0.1517 - val_acc: 0.9405\n",
      "Epoch 19/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1569 - acc: 0.9387 - val_loss: 0.1510 - val_acc: 0.9417\n",
      "Epoch 20/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1557 - acc: 0.9390 - val_loss: 0.1499 - val_acc: 0.9421\n",
      "Epoch 21/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1547 - acc: 0.9396 - val_loss: 0.1496 - val_acc: 0.9421\n",
      "Epoch 22/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1539 - acc: 0.9401 - val_loss: 0.1501 - val_acc: 0.9410\n",
      "Epoch 23/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1530 - acc: 0.9401 - val_loss: 0.1495 - val_acc: 0.9415\n",
      "Epoch 00022: early stopping\n"
     ]
    }
   ],
   "source": [
    "top_model_weights_path = '../models/vgg16_bottleneck_fc_model_sgd_temp.h5'\n",
    "train_data = np.load(open('../models/bottleneck_features_train.npy', 'rb'))\n",
    "train_labels = np.load(open('../models/bottleneck_labels_train.npy', 'rb'))\n",
    "train_labels = train_labels[:len(train_data)]\n",
    "print(train_data.shape)\n",
    "validation_data = np.load(open('../models/bottleneck_features_validation.npy', 'rb'))\n",
    "validation_labels = np.load(open('../models/bottleneck_labels_validation.npy', 'rb'))\n",
    "validation_labels = validation_labels[:len(validation_data)]\n",
    "\n",
    "model = Sequential()\n",
    "print(train_data.shape[1:])\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(17, activation='sigmoid'))\n",
    "    \n",
    "sgd_1 = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd_2 = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd_3 = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model.compile(optimizer='adam',\n",
    "     #             loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=sgd_1, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.0002, patience=2, verbose=1, mode='auto')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(validation_data.shape)\n",
    "print(validation_labels.shape)\n",
    "model.fit(train_data, train_labels,\n",
    "              epochs=300,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "             callbacks=[es])\n",
    "model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Tweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34304 samples, validate on 5888 samples\n",
      "Epoch 1/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1493 - acc: 0.9414 - val_loss: 0.1459 - val_acc: 0.9435\n",
      "Epoch 2/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1492 - acc: 0.9416 - val_loss: 0.1459 - val_acc: 0.9433\n",
      "Epoch 3/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1494 - acc: 0.9416 - val_loss: 0.1460 - val_acc: 0.9435\n",
      "Epoch 4/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1494 - acc: 0.9415 - val_loss: 0.1457 - val_acc: 0.9435\n",
      "Epoch 5/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1490 - acc: 0.9416 - val_loss: 0.1456 - val_acc: 0.9433\n",
      "Epoch 6/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1490 - acc: 0.9416 - val_loss: 0.1455 - val_acc: 0.9436\n",
      "Epoch 7/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1490 - acc: 0.9418 - val_loss: 0.1453 - val_acc: 0.9436\n",
      "Epoch 8/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1491 - acc: 0.9418 - val_loss: 0.1458 - val_acc: 0.9434\n",
      "Epoch 9/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1488 - acc: 0.9417 - val_loss: 0.1454 - val_acc: 0.9437\n",
      "Epoch 10/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1488 - acc: 0.9416 - val_loss: 0.1455 - val_acc: 0.9435\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1, mode='auto')\n",
    "model.compile(optimizer=sgd_2, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels,\n",
    "              epochs=300,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "            callbacks=[es])\n",
    "model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34304 samples, validate on 5888 samples\n",
      "Epoch 1/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1494 - acc: 0.9413 - val_loss: 0.1461 - val_acc: 0.9430\n",
      "Epoch 2/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1496 - acc: 0.9415 - val_loss: 0.1461 - val_acc: 0.9430\n",
      "Epoch 3/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1496 - acc: 0.9414 - val_loss: 0.1460 - val_acc: 0.9431\n",
      "Epoch 4/300\n",
      "34304/34304 [==============================] - 8s - loss: 0.1497 - acc: 0.9414 - val_loss: 0.1461 - val_acc: 0.9431\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1, mode='auto')\n",
    "model.compile(optimizer=sgd_3, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels,\n",
    "              epochs=300,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "            callbacks=[es])\n",
    "model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difine VGG model\n",
    "- Load top layer parameters\n",
    "- Load VGG parameters\n",
    "- Define optimiser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/dist-packages/ipykernel_launcher.py:32: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n"
     ]
    }
   ],
   "source": [
    "# build the VGG16 network\n",
    "top_model_weights_path = '../models/vgg16_bottleneck_fc_model_sgd.h5'\n",
    "batch_size = 128\n",
    "input_tensor = Input(shape=(150,150,3))\n",
    "base_model = applications.VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
    "#last = base_model.output\n",
    "print('Model loaded.')\n",
    "#print(last)\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "#x = Flatten(input_shape=(4,4,512))(last)\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(4096, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(4096, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(17, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "#x = Flatten()(last)\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "#preds = Dense(200, activation='softmax')(x)\n",
    "\n",
    "#model = Model(initial_model.input, preds)\n",
    "model = Model(input= base_model.input, output= top_model(base_model.output))\n",
    "#model.add(top_model)\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "#for layer in model.layers[:25]:\n",
    "#    layer.trainable = False\n",
    "    \n",
    "    \n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=1e-3, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Data aug from simple attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "train_generator = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "train_genorator_loaded = []\n",
    "\n",
    "file_path = '../models/vgg_cnn_weights_{epoch:02d}_{val_loss:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=0, save_best_only=False, \n",
    "                                save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1, mode='auto')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "valid_generator = datagen.flow(\n",
    "        x_valid, y_valid,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "229/228 [==============================] - 913s - loss: 0.1457 - acc: 0.9434 - val_loss: 0.1261 - val_acc: 0.9518\n",
      "Epoch 2/100\n",
      "229/228 [==============================] - 893s - loss: 0.1317 - acc: 0.9496 - val_loss: 0.1221 - val_acc: 0.9540\n",
      "Epoch 3/100\n",
      "166/228 [====================>.........] - ETA: 230s - loss: 0.1271 - acc: 0.9514"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train) / batch_size, epochs=100, validation_data=valid_generator,\n",
    "                    validation_steps = len(x_valid) / batch_size,\n",
    "                    callbacks = [model_checkpoint, es], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "229/228 [==============================] - 899s - loss: 0.1038 - acc: 0.9603 - val_loss: 0.0989 - val_acc: 0.9620\n",
      "Epoch 22/100\n",
      "229/228 [==============================] - 899s - loss: 0.1031 - acc: 0.9609 - val_loss: 0.0988 - val_acc: 0.9622\n",
      "Epoch 23/100\n",
      "229/228 [==============================] - 899s - loss: 0.1033 - acc: 0.9609 - val_loss: 0.0987 - val_acc: 0.9621\n",
      "Epoch 24/100\n",
      "229/228 [==============================] - 899s - loss: 0.1032 - acc: 0.9608 - val_loss: 0.0988 - val_acc: 0.9620\n",
      "Epoch 25/100\n",
      "229/228 [==============================] - 899s - loss: 0.1036 - acc: 0.9607 - val_loss: 0.0990 - val_acc: 0.9621\n",
      "Epoch 26/100\n",
      "229/228 [==============================] - 899s - loss: 0.1028 - acc: 0.9610 - val_loss: 0.0989 - val_acc: 0.9619\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b715f5860>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train) / batch_size, epochs=100, validation_data=valid_generator,\n",
    "                    validation_steps = len(x_valid) / batch_size,\n",
    "                    callbacks = [model_checkpoint, es], verbose=1, initial_epoch = 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "135/134 [==============================] - 263s - loss: 0.1575 - acc: 0.9386 - val_loss: 0.1457 - val_acc: 0.9435\n",
      "Epoch 2/10\n",
      " 16/134 [==>...........................] - ETA: 135s - loss: 0.1549 - acc: 0.9402"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5fb6c8f2e932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     callbacks = [model_checkpoint, es], verbose=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.4/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.4/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.4/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = '../models/vgg_cnn_weights_{epoch:02d}_{val_loss:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=0, save_best_only=False, \n",
    "                                save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=1e-1, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=0.5 * len(x_train) / batch_size, epochs=10, validation_data=valid_generator,\n",
    "                    validation_steps = len(x_valid) / batch_size,\n",
    "                    callbacks = [model_checkpoint, es], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91713348386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "p_valid = model.predict(x_valid/255., batch_size=128)\n",
    "print(fbeta_score(y_valid, np.array(p_valid) > 0.2, beta=2, average='samples'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
